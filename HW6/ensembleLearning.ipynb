{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a8fa5dab-9a8c-4c9b-93d8-ca402474d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv('train_and_test2.csv')\n",
    "data = data.drop(columns=['zero'])\n",
    "for i in range(1, 19):\n",
    "    data = data.drop(columns=[f'zero.{i}'])\n",
    "data.rename(columns={'2urvived': 'Survived'}, inplace=True)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "     #use Gini  or Entropy to measure split quality. Recursively splits the dataset to maximize information gain.\n",
    " \n",
    "    class Node:\n",
    "        def __init__(self, *, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def __init__(self, criterion='gini', max_depth=None, min_samples_leaf=1):\n",
    "        if criterion not in ('gini', 'entropy'):\n",
    "            raise ValueError(\"criterion must be 'gini' or 'entropy'\")\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = float('inf') if max_depth is None else int(max_depth)\n",
    "        self.min_samples_leaf = max(1, int(min_samples_leaf))\n",
    "        self.tree_ = None\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        return 1.0 - np.sum(probs ** 2)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "    def _impurity(self, y):\n",
    "        return self._gini(y) if self.criterion == 'gini' else self._entropy(y)\n",
    "\n",
    "    def _majority_class(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        return np.argmax(counts)\n",
    "    \n",
    "#identify the best split we can do by splitting data to left and right node\n",
    "    def _best_split(self, X, y):\n",
    "        m, n_features = X.shape\n",
    "        if m < 2 * self.min_samples_leaf:\n",
    "            return None, None, None, None, 0.0\n",
    "\n",
    "        parent_impurity = self._impurity(y)\n",
    "        best_gain = 0.0\n",
    "        best_feat = None\n",
    "        best_thresh = None\n",
    "        best_left_mask = None\n",
    "        best_right_mask = None\n",
    "\n",
    "        max_features = max(1, int(np.sqrt(n_features)))\n",
    "        feature_indices = np.random.choice(n_features, max_features, replace=False)\n",
    "\n",
    "        for feat in feature_indices:\n",
    "            values = X[:, feat]\n",
    "            sorted_idx = np.argsort(values)\n",
    "            sorted_vals = values[sorted_idx]\n",
    "\n",
    "            for i in range(1, m):\n",
    "                if sorted_vals[i] == sorted_vals[i - 1]:\n",
    "                    continue\n",
    "                thresh = (sorted_vals[i] + sorted_vals[i - 1]) / 2.0\n",
    "                left_mask = values <= thresh\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if left_mask.sum() < self.min_samples_leaf or right_mask.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                gain = parent_impurity - (\n",
    "                    (left_mask.sum() / m) * self._impurity(y[left_mask]) +\n",
    "                    (right_mask.sum() / m) * self._impurity(y[right_mask])\n",
    "                )\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat = feat\n",
    "                    best_thresh = thresh\n",
    "                    best_left_mask = left_mask\n",
    "                    best_right_mask = right_mask\n",
    "\n",
    "        return best_feat, best_thresh, best_left_mask, best_right_mask, best_gain\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if depth >= self.max_depth or len(y) <= self.min_samples_leaf or len(set(y)) == 1:\n",
    "            return DecisionTree.Node(value=self._majority_class(y))\n",
    "\n",
    "        feat, thresh, left_mask, right_mask, gain = self._best_split(X, y)\n",
    "        if feat is None or gain <= 0.0:\n",
    "            return DecisionTree.Node(value=self._majority_class(y))\n",
    "\n",
    "        left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        return DecisionTree.Node(feature_index=feat, threshold=thresh, left=left, right=right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        self.tree_ = self._build_tree(np.array(X), np.array(y))\n",
    "        return self\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature_index] <= node.threshold else node.right\n",
    "        return node.value\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        return np.array([self._predict_one(row, self.tree_) for row in np.array(X)])\n",
    "\n",
    "    def confusion_matrix(self, y_true, y_pred):\n",
    "        classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "        class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "        for actual, pred in zip(y_true, y_pred):\n",
    "            matrix[class_to_index[actual]][class_to_index[pred]] += 1\n",
    "        return matrix\n",
    "\n",
    "    def classification_metrics(self, cm):\n",
    "        TP = cm[1, 1]\n",
    "        TN = cm[0, 0]\n",
    "        FP = cm[0, 1]\n",
    "        FN = cm[1, 0]\n",
    "\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1_score = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "04dba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(X, y, n_bootstraps=10, random_state=None):\n",
    "\n",
    "    #Random sampling with replacement to create multiple datasets and train with each and have more train data.\n",
    "     \n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    bootstraps = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_boot = X.iloc[indices] if isinstance(X, pd.DataFrame) else X[indices]\n",
    "        y_boot = y.iloc[indices] if isinstance(y, pd.Series) else y[indices]\n",
    "        bootstraps.append((X_boot, y_boot))\n",
    "    return bootstraps\n",
    "\n",
    "\n",
    "def majority_vote(predictions):\n",
    "   \n",
    "    #Each tree votes for a class.The final prediction is the most common vote.\n",
    "   \n",
    "    n_estimators, n_samples = predictions.shape\n",
    "    final_preds = np.zeros(n_samples, dtype=int)\n",
    "    for i in range(n_samples):\n",
    "        counts = np.bincount(predictions[:, i].astype(int))\n",
    "        final_preds[i] = np.argmax(counts)\n",
    "    return final_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a7626385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for model1:\n",
      "[[175  14]\n",
      " [ 42  31]]\n",
      "confusion matrix for model2:\n",
      "[[174  15]\n",
      " [ 43  30]]\n",
      "-----------answer table-----------\n",
      "            Model  Accuracy  Precision   Recall  F1 Score\n",
      "   Model 1 - Gini  0.786260   0.688889 0.424658  0.525424\n",
      "Model 2 - Entropy  0.778626   0.666667 0.410959  0.508475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = ['Age', 'Fare', 'Sex', 'sibsp', 'Parch', 'Pclass', 'Embarked']\n",
    "X = data[features]\n",
    "Y = data['Survived']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "#  Model 1 \n",
    "bootstraps = bootstrap_sampling(x_train, y_train, n_bootstraps=50, random_state=42)\n",
    "all_pred = []\n",
    "for boot_x, boot_y in bootstraps:\n",
    "    tree_model = DecisionTree('gini', 25, 25)\n",
    "    tree_model.fit(boot_x, boot_y)\n",
    "    pred = tree_model.predict(x_test)\n",
    "    all_pred.append(pred)\n",
    "\n",
    "all_pred = np.array(all_pred)\n",
    "final_y_pred = majority_vote(all_pred)\n",
    "\n",
    "cm = tree_model.confusion_matrix(y_test, final_y_pred)\n",
    "metrics = tree_model.classification_metrics(cm)\n",
    "results.append((\"Model 1 - Gini\", metrics))\n",
    "\n",
    "print('confusion matrix for model1:')\n",
    "print(cm)\n",
    "# Model 2 \n",
    "bootstraps = bootstrap_sampling(x_train, y_train, n_bootstraps=50, random_state=42)\n",
    "all_pred = []\n",
    "for boot_x, boot_y in bootstraps:\n",
    "    tree_model = DecisionTree('entropy', 25, 25)\n",
    "    tree_model.fit(boot_x, boot_y)\n",
    "    pred = tree_model.predict(x_test)\n",
    "    all_pred.append(pred)\n",
    "\n",
    "all_pred = np.array(all_pred)\n",
    "final_y_pred = majority_vote(all_pred)\n",
    "\n",
    "cm = tree_model.confusion_matrix(y_test, final_y_pred)\n",
    "metrics = tree_model.classification_metrics(cm)\n",
    "results.append((\"Model 2 - Entropy\", metrics))\n",
    "\n",
    "print('confusion matrix for model2:')\n",
    "print(cm)\n",
    "\n",
    "df_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": m[\"accuracy\"],\n",
    "        \"Precision\": m[\"precision\"],\n",
    "        \"Recall\": m[\"recall\"],\n",
    "        \"F1 Score\": m[\"f1_score\"]\n",
    "    }\n",
    "    for name, m in results\n",
    "])\n",
    "\n",
    "print('-----------answer table-----------')\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
